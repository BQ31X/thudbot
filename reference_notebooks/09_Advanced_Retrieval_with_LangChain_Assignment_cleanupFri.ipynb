{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### API key management\n",
        "\n",
        "### Reminder: Place .env file inside the root of the project folder so when calling the below from inside the notebook it should find the .env fule and load it inside the notebook environment\n",
        "### PLEASE ADD THIS `.env` FILE TO YOUR PROJECT'S `.gitignore` file before committing and pushing the changes to your remote repo, as it contains API Keys and Secrets in it\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\"../.env\")\n",
        "\n",
        "print(\"OPENAI_API_KEY\" in os.environ)\n",
        "print(\"LANGCHAIN_API_KEY\" in os.environ)\n",
        "print(\"TAVILY_API_KEY\" in os.environ)\n",
        "print(\"RAGAS_API_KEY\" in os.environ)\n",
        "print(\"ANTHROPIC_API_KEY\" in os.environ)\n",
        "print(\"Cohere_API_KEY\" in os.environ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#import getpass\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "#os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "#embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
        "#chat_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "#### âœ… Answer #1:\n",
        "BM25 would be better when looking for very specific and unique text such as serial numbers, model numbers, error codes, policy document names, etc. This is because BM25 ranks results based on exact matches. Embedding-based retrievers prioritize semantic similarity and may not find unique values, especially if they lack semantic meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "#### ðŸŽ¥âœ… Answer #2:\n",
        "\n",
        "User queries may not always be well formatted. Often, the hardest part of solving a problem is having a clear problem statement. For an analogy, think of an audience member asking a question of a presenter, and the presenter responding with: \"I don't follow your question, could you state that another way, please\". Having an LLM reformulate the questiion in different ways, increases the chances that the retriever will match documents that might have expressed the relevant idea with different wording."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding = CohereEmbeddings(model=\"embed-english-v3.0\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "#### âœ… Answer #3:\n",
        "\n",
        "Short answer is that semantic chunking in this case would group many of the short, similar sentences together. And, this would probably be desirable. But, if the bar is too low, it might group together sentences that are only superficially similar. To adjust this, you would raise the threshold, e.g. in this case, the percentile value that triggers a split.\n",
        "\n",
        "I also wanted to make sure I understood the math and process behind semantic chunking. \n",
        "\n",
        "\n",
        "The Process\n",
        "First, split document into sentences (langchain uses sentence-based splitter, e.g. nltk.sent_tokenizer)\n",
        "Then do pairwise comparisons of the semantic distance between sentences.\n",
        "Then group sentences if they are \"close\" in semantic meaning, and split to a new chunk when the semantic distance to the previous sentence exceeds the threshold. \n",
        "\n",
        "The Math\n",
        "\n",
        "Percentile: \n",
        "Split when the semantic distance between two adjacent sentences is bigger than the n-th percentile of all adjacent-pair distances in the document. (Typical starting threshold: **75th or 80th percentile**. Lower the threshold to get smaller chunks (e.g. if topic shifts frequently) or raise the percentil to get fewer larger chunks)\n",
        "\n",
        "Standard Deviation: \n",
        "Split when the semantic distance between two adjacent sentences exceeds the mean by nore than n times the stddev of all adjacent-pair distances in the document. (Typical value: **n = 1.5**; threshold = **Mean + 1.5 Ã— StdDev**)\n",
        "\n",
        "Interquartile:\n",
        "Split when the distance falls above the Q3 percentile by some factor, \"n\", of the \"interquartile range\" (IQR). IQR is the Q3 percentile - Q1 percentile. Example: threshold = Q3 + n x IQR (Typical value: **n = 1.5**; threshold = **Q3 + 1.5 Ã— IQR**)\n",
        "\n",
        "Gradient:\n",
        "This is kind of like a first order derivative, compared to the others. It essentially measures the velocity of change between the sentences (as opposed ot the distance), triggering a split when there is a sudden change. To do this, after calculating the distances between each set of pairs, you then calculate the gradients (the distances between the distances). The rule would be to split whenever the gradient exceeds a threshold, **N**.  \n",
        "(Typical value: **N = 0.10**)\n",
        "\n",
        "ðŸŽ¥ See example [here](semantic_chunking_examples.md) that I worked out with ChatGPT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Study Note: Documentation\n",
        "I'm going to document the heck out of this.\n",
        "If I track every step that I do, I'll know what to say in my Loom video ðŸ¤ž\n",
        "First off, I'm eager and nervous to try this.\n",
        "I'm pretty sure I can find all the code I need in notebooks from session 7 & 8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Study Note: Design Idea\n",
        "Activity 1 instructions seem pretty clear, but I ran it by ChatGPT to confirm.\n",
        "I need to \n",
        "1. Create synth dataset for test case\n",
        "2. Evaluate 5 retreival approaches: Naive, BM25, Multi-Query, Parent Doc, Ensemble with Ragas\n",
        "    a. run each retriever with the golden dataset\n",
        "    b. evaluate each run with Ragas\n",
        "3. Then I'll have to do something with LangSmith to get the latency and cost, but I'll think about that later\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Create a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will find the ragas code from notebook 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, I'll do the nltk thing. I still don't undertsand it, but the comment in 7 said it would prevent (mac related) os errors.\n",
        "It worked there, so I will keep it.\n",
        "First stumbling block: it failed. But, I figured out why pretty quick. This notebook didn't install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Study Note: Environment setup\n",
        "Till now, I've just been relying on uv sync, and not thinking much about dependencies.\n",
        "So, I took a look at the project.toml for this notebook vs 7. Lots of stuff in 7 that isn't here, and I think I'm going to need it.\n",
        "But, I don't want to break anything, so, I'll just add things as I run into needing them (starting with nltk). Also, I'll just install via terminal first, and if nothing breaks, I'll update the toml file later.\n",
        "Clearly, I'm also going to need ragas\n",
        "Ran this command:  uv pip install nltk ragas==0.2.10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Study Notes: To-dos\n",
        "Track thinkgs here that are pending\n",
        "update toml with nltk and ragas (see above)\n",
        "update toml with rapidfuzz (see below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That worked. Baby steps!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'm going to end up doing something with LangSmith, so I'll steal the next cell from earlier notebooks (plus a print statement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Number9 Evaluation 3\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I'll import stuff and set the llm \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking the 'abstracted SDG' from HW7\n",
        "I ran it just as it was in HW7, and I got an error, because there was no such thing as \"docs\", so i figured out that I needed to put in \"loan_complaint_data\"\n",
        "Pretty happy that I figured that out, with only a slight nudge from ChatGPT\n",
        "It failed, with a nice clear error message that I needed to install \"rapidfuzz\".\n",
        "\n",
        "Don't know why I need rapidfuzz now, when we didn't need it in HW7. \n",
        "I suppose it is because of the structure of the data.\n",
        "ChatGPT gave me some other hogwash that I don't believe. I'm sticking with my guess, and it doesn't really matter.\n",
        "I installed rapidfuzz (in terminal window; I'll need to add a cell or put it in the toml file later)<< added to toml!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "golden_dataset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow, that worked!\n",
        "Now, I need to see the dataset, so I'm stealing the next cell from notebook 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am gobsmacked that I got this working in under an hour (not counting annotations) Insert excessive emojis here: âœ…â¤ï¸ðŸ¥³ðŸŽ‰\n",
        "Time to (commit the code), take a victory lap, and call it a night."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "golden_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coming back to work on this, after completing Hw10, so now I have to reset **my** context.\n",
        "I went ahead and added nltk and ragas to project.toml, so I can clean up my 'technical debt'\n",
        "Reran the notebook, and it all worked.\n",
        "Now, I ended up with 10 new questions - \n",
        "\n",
        "Going to stash this dataset into a json, just in case I need it back later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"goldendataset.json\", \"w\") as f:\n",
        "    json.dump([sample.model_dump() for sample in golden_dataset], f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "some code for later, just in case I do need to get back then golden_data from the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the golden dataset\n",
        "with open(\"goldendataset.json\", \"r\") as f:\n",
        "    golden_dataset = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(golden_dataset)} test samples\")\n",
        "print(f\"ðŸ“ First question: {golden_dataset[0]['eval_sample']['user_input']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load questions for testing your retrievers\n",
        "with open(\"goldendataset.json\", \"r\") as f:\n",
        "    golden_data = json.load(f)\n",
        "\n",
        "def run_retriever_for_ragas(retriever_chain, golden_data):\n",
        "    \"\"\"Run retriever and format for Ragas evaluation\"\"\"\n",
        "    outputs = []\n",
        "    \n",
        "    for item in golden_data:\n",
        "        question = item[\"eval_sample\"][\"user_input\"]\n",
        "        reference = item[\"eval_sample\"][\"reference\"]\n",
        "        \n",
        "        # Run your retriever\n",
        "        result = retriever_chain.invoke({\"question\": question})\n",
        "        \n",
        "        outputs.append({\n",
        "            \"user_input\": question,\n",
        "            \"reference\": reference,\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"retrieved_contexts\": [doc.page_content for doc in result[\"context\"]]\n",
        "        })\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "# Use with your retrievers\n",
        "# ragas_data = run_retriever_for_ragas(naive_retrieval_chain, golden_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(golden_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reimporting golden data set from json after a restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load questions for testing your retrievers\n",
        "with open(\"goldendataset.json\", \"r\") as f:\n",
        "    golden_data = json.load(f)\n",
        "\n",
        "def run_retriever_on_dataset(name, retriever_chain, golden_data):\n",
        "    \"\"\"Run retriever and format for Ragas evaluation - consistent with your existing function\"\"\"\n",
        "    print(f\"Running {name} on golden dataset\")\n",
        "    outputs = []\n",
        "    \n",
        "    for item in golden_data:\n",
        "        question = item[\"eval_sample\"][\"user_input\"]\n",
        "        reference = item[\"eval_sample\"][\"reference\"]\n",
        "        \n",
        "        # Run your retriever\n",
        "        response = retriever_chain.invoke({\"question\": question})\n",
        "        \n",
        "        outputs.append({\n",
        "            \"user_input\": question,\n",
        "            \"reference\": reference,\n",
        "            \"response\": response[\"response\"].content if hasattr(response[\"response\"], \"content\") else response[\"response\"],\n",
        "            \"retrieved_contexts\": [ctx.page_content for ctx in response[\"context\"]],\n",
        "            \"retriever_name\": name\n",
        "        })\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "Evaluate 5 retreival approaches: Naive, BM25, Multi-Query, Parent Doc, Ensemble with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.1 Run retreivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step: Run a retriever with the golden data set\n",
        "Starting slowly, with one retriever and saving the outputs\n",
        "\n",
        "here are the names of the retrieval chains, from above, for my reference\n",
        "naive_retrieval_chain\n",
        "bm25_retrieval_chain\n",
        "contextual_compression_retrieval_chain\n",
        "multi_query_retrieval_chain\n",
        "parent_document_retrieval_chain\n",
        "ensemble_retrieval_chain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "first, just try one retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bm25_outputs = run_retriever_on_dataset(\"bm25_thing\", bm25_retrieval_chain, golden_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#first I'll try to run, with BM25 (hard-coded)\n",
        "def run_retriever_on_dataset(retriever_chain, golden_dataset): #since this version is hard-coded, it never actually uses the retriever_chain parameter\n",
        "    outputs = []\n",
        "\n",
        "    for test_row in golden_dataset:\n",
        "        print(test_row)\n",
        "        response = bm25_retrieval_chain.invoke({\n",
        "            \"question\": test_row.eval_sample.user_input})\n",
        "        outputs.append({\n",
        "            \"user_input\": test_row.eval_sample.user_input,\n",
        "            \"reference\": test_row.eval_sample.reference,\n",
        "            \"response\": response[\"response\"].content if hasattr(response[\"response\"], \"content\") else response[\"response\"],\n",
        "            \"retrieved_contexts\": [ctx.page_content for ctx in response[\"context\"]],\n",
        "        })\n",
        "\n",
        "    return outputs\n",
        "\n",
        "bm25_outputs = run_retriever_on_dataset(bm25_retrieval_chain, golden_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "bm25_df = pd.DataFrame(bm25_outputs)\n",
        "bm25_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, convert output to a pandas.DataFrame first, and then into a Ragas EvaluationDataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ragas import EvaluationDataset\n",
        "\n",
        "# Step 1: Convert to DataFrame\n",
        "bm25_df = pd.DataFrame(bm25_outputs)\n",
        "\n",
        "# Step 2: Convert to Ragas-compatible EvaluationDataset\n",
        "bm25_eval_dataset = EvaluationDataset.from_pandas(bm25_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate first retriever (BM25)\n",
        "borrowing some more code fron HW8, for the eval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=bm25_eval_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Party time!\n",
        "\n",
        "I ran an evaluation for 1 retriever; time for another victory lap. \n",
        "first, stash the output for safe-keeping (since it takes 6 minutes), then figure out how to externalize the retriever name so I can loop through them all\n",
        "\n",
        "BM25 output from first run:\n",
        "{'context_recall': 0.8417, 'faithfulness': 0.7594, 'factual_correctness': 0.5620, 'answer_relevancy': 0.6619, 'context_entity_recall': 0.4776, 'noise_sensitivity_relevant': 0.2619}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "instead of updating the run_retriever_on_dataset definition from above, I'll make a newer copy below, just to continue to trace my process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define retriever names, to use with the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrievers = {\n",
        "    #\"bm25\": bm25_retrieval_chain, commented out because it was already run\n",
        "    \"naive\": naive_retrieval_chain,\n",
        "    \"multi_query\": multi_query_retrieval_chain,\n",
        "    \"parent_doc\": parent_document_retrieval_chain,\n",
        "    \"ensemble\": ensemble_retrieval_chain,\n",
        "    \"contextual_compression\": contextual_compression_retrieval_chain,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#updated version of the code, with the retriever name externalized\n",
        "# import time\n",
        "\n",
        "def run_retriever_on_dataset(name, retriever_chain, golden_dataset):\n",
        "    print(f\"Running {name} on golden dataset\")\n",
        "    outputs = []\n",
        "\n",
        "    for test_row in golden_dataset:\n",
        "        response = retriever_chain.invoke({\n",
        "            \"question\": test_row.eval_sample.user_input})\n",
        "        outputs.append({\n",
        "            \"user_input\": test_row.eval_sample.user_input,\n",
        "            \"reference\": test_row.eval_sample.reference,\n",
        "            \"response\": response[\"response\"].content if hasattr(response[\"response\"], \"content\") else response[\"response\"],\n",
        "            \"retrieved_contexts\": [ctx.page_content for ctx in response[\"context\"]],\n",
        "            \"retriever_name\": name #this is the new addition for being able to keep track of the retriever name later\n",
        "        })\n",
        "\n",
        "\n",
        "    #    # Add delay between requests\n",
        "    #     if i < len(golden_dataset) - 1:  # Don't sleep after last item\n",
        "    #         print(f\"  Waiting 2 seconds before next request...\")\n",
        "    #         time.sleep(2)  # Adjust this value as needed\n",
        "\n",
        "    return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "next cell looked like a good idea, but it was overengineerd and didn't give what I wanted / needed for output format\n",
        "something about dictionary of lists vs list of dictionaries \n",
        "and anyway, I would have had to split up the output because I don't want to run 40 minutes worth of evals in one cell\n",
        "this tends to happen when I listen to the AI too much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# all_outputs = {} #initializing the dictionary to store the outputs\n",
        "\n",
        "# for name, chain in retrievers.items():\n",
        "#     print(f\"Running: {name}\")\n",
        "#     outputs = run_retriever_on_dataset(name, chain, golden_dataset)\n",
        "#     all_outputs[name] = outputs #adding the outputs to a single dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this is the brute force version, with each retriever run separately, which works for me especially since I might need to run some of the retrievers individually, or multiple times\n",
        "which I will now need to do, because contextual compression failed with a rate limit error. I will retry with Cohere production key.\n",
        "(Production key worked ðŸ‘)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bm25_outputs = run_retriever_on_dataset(\"bm25\", bm25_retrieval_chain, golden_dataset) #this was the first version, hard-coded\n",
        "# naive_outputs = run_retriever_on_dataset(\"naive\", naive_retrieval_chain, golden_dataset)\n",
        "multi_query_outputs = run_retriever_on_dataset(\"multi_query\", multi_query_retrieval_chain, golden_dataset)\n",
        "# parent_doc_outputs = run_retriever_on_dataset(\"parent_doc\", parent_document_retrieval_chain, golden_dataset)\n",
        "# ensemble_outputs = run_retriever_on_dataset(\"ensemble\", ensemble_retrieval_chain, golden_dataset)\n",
        "# contextual_compression_outputs = run_retriever_on_dataset(\"contextual_compression\", contextual_compression_retrieval_chain, golden_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to do some validation. \n",
        "Next code cell is straiht from ChatGPT. \n",
        "this is the kind of stuff it is pretty good at, and I don't feel like I need to dive into the details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_keys = {\"user_input\", \"reference\", \"response\", \"retrieved_contexts\"}\n",
        "\n",
        "for name, output in [\n",
        "    (\"bm25\", bm25_outputs),\n",
        "    (\"naive\", naive_outputs),\n",
        "    (\"multi_query\", multi_query_outputs),\n",
        "    (\"parent_doc\", parent_doc_outputs),\n",
        "    (\"ensemble\", ensemble_outputs),\n",
        "    (\"contextual_compression\", contextual_compression_outputs),\n",
        "]:\n",
        "    if not isinstance(output, list):\n",
        "        print(f\"{name}: âŒ Not a list\")\n",
        "        continue\n",
        "\n",
        "    if not output:\n",
        "        print(f\"{name}: âš ï¸ Empty list\")\n",
        "        continue\n",
        "\n",
        "    sample = output[0]\n",
        "    if not isinstance(sample, dict):\n",
        "        print(f\"{name}: âŒ First item is not a dict\")\n",
        "        continue\n",
        "\n",
        "    missing = expected_keys - set(sample.keys())\n",
        "    if missing:\n",
        "        print(f\"{name}: âŒ Missing keys: {missing}\")\n",
        "    else:\n",
        "        print(f\"{name}: âœ… Format looks good ({len(output)} samples)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, converting the outputs to evaluation datasets. Probably there's a more efficient approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ragas import EvaluationDataset\n",
        "\n",
        "# Step 1: Convert to DataFrame\n",
        "# bm25_df = pd.DataFrame(bm25_outputs)\n",
        "naive_df = pd.DataFrame(naive_outputs)\n",
        "multi_query_df = pd.DataFrame(multi_query_outputs)\n",
        "parent_doc_df = pd.DataFrame(parent_doc_outputs)\n",
        "ensemble_df = pd.DataFrame(ensemble_outputs)\n",
        "contextual_compression_df = pd.DataFrame(contextual_compression_outputs)\n",
        "\n",
        "# Step 2: Convert to Ragas-compatible EvaluationDataset\n",
        "# bm25_eval_dataset = EvaluationDataset.from_pandas(bm25_df)\n",
        "naive_eval_dataset = EvaluationDataset.from_pandas(naive_df)\n",
        "multi_query_eval_dataset = EvaluationDataset.from_pandas(multi_query_df)\n",
        "parent_doc_eval_dataset = EvaluationDataset.from_pandas(parent_doc_df)\n",
        "ensemble_eval_dataset = EvaluationDataset.from_pandas(ensemble_df)\n",
        "contextual_compression_eval_dataset = EvaluationDataset.from_pandas(contextual_compression_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "putting all the ragas imports together, here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next cell is same as I used above for BM25, but now putting it into a loop to run all the evals together\n",
        "hoping this:\n",
        "\n",
        "results[name] = result give me a separate, named, result set for each retriever ðŸ¤ž"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_run_config = RunConfig(timeout=600)\n",
        "results = {}\n",
        "\n",
        "datasets = [\n",
        "    (\"bm25\", bm25_eval_dataset),\n",
        "    (\"naive\", naive_eval_dataset),\n",
        "    (\"multi_query\", multi_query_eval_dataset),\n",
        "    (\"parent_doc\", parent_doc_eval_dataset),\n",
        "    (\"ensemble\", ensemble_eval_dataset),\n",
        "    (\"contextual_compression\", contextual_compression_eval_dataset)\n",
        "]\n",
        "\n",
        "for name, dataset in datasets:\n",
        "    print(f\"Evaluating: {name}\")\n",
        "    try:\n",
        "        result = evaluate(\n",
        "            dataset=dataset,\n",
        "            metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "            llm=evaluator_llm,\n",
        "            run_config=custom_run_config\n",
        "        )\n",
        "        results[name] = result\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during {name}: {e}\")\n",
        "        results[name] = None  # or skip entirely\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Party time again!\n",
        "The evals all ran. took 56 minutes. Maybe I shouldn't have done them all at once?\n",
        "I had to increase the timeout; I was getting too many errors at 360, so upped it to 10 minutes.\n",
        "Ensemble and multi-query still generated too many timeout errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the next cell to make sure I really got the results, the way I expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results[\"contextual_compression\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That worked, so lets try looping them all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in results:\n",
        "    print(f\"\\n{name.upper()} RESULTS:\")\n",
        "    print(results[name])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Got all the results; sav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success\n",
        "I got the Ragas results! I saved them to rags_results_1.\n",
        "I also need cost and latency, which I can get from LangGr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Get latency and cost\n",
        "Now I need to use Langchain.\n",
        "Turns out, I should have thought of this sooner, because I probably could have used the same retriever runs to trace with LangChain and also use as output for Ragas. Oh, well, they don't take so long to run. \n",
        "\n",
        "Main thing is I have to figure out how to configure tracing. I'll root around in homework 7, and then bug one of the AI assistants if that doesn't work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Langchain setup (tried to do this above, but let's get everything in one place and get it working)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Number9 Evaluation 4\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## LangSmith Tracing Setup Guide\n",
        "\n",
        "Let's get LangSmith tracing working to capture cost and latency data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#magic France test cell\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.tracers import LangChainTracer\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "#llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_template(\"What is the capital of France?\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Attach a tracer manually (failsafe if env vars don't take)\n",
        "tracer = LangChainTracer()\n",
        "chain_with_tracing = chain.with_config({\"callbacks\": [tracer]})\n",
        "\n",
        "# Invoke chain\n",
        "response = chain_with_tracing.invoke({})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_outputs_traced = run_retriever_on_dataset(\"bm25\", bm25_retrieval_chain, golden_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Verify your environment variables are set\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Make sure your .env file is loaded\n",
        "load_dotenv(dotenv_path=\"../.env\")\n",
        "\n",
        "# Check if all required keys are present\n",
        "required_keys = [\"OPENAI_API_KEY\", \"LANGCHAIN_API_KEY\", \"LANGCHAIN_TRACING_V2\"]\n",
        "for key in required_keys:\n",
        "    if key in os.environ:\n",
        "        print(f\"âœ… {key}: SET\")\n",
        "    else:\n",
        "        print(f\"âŒ {key}: MISSING\")\n",
        "\n",
        "# Set the project name for organizing traces\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Advanced-Retrieval-Evaluation\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "print(f\"\\nðŸŽ¯ LangSmith Project: {os.environ.get('LANGCHAIN_PROJECT')}\")\n",
        "print(f\"ðŸ“Š Tracing Enabled: {os.environ.get('LANGCHAIN_TRACING_V2')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ” Comprehensive LangSmith Diagnostic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"ðŸ”§ LANGSMITH DIAGNOSTIC REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Check environment variables\n",
        "print(\"\\n1ï¸âƒ£ ENVIRONMENT VARIABLES:\")\n",
        "load_dotenv(dotenv_path=\"../.env\")\n",
        "\n",
        "env_vars = {\n",
        "    \"LANGCHAIN_API_KEY\": os.environ.get(\"LANGCHAIN_API_KEY\", \"NOT SET\"),\n",
        "    \"LANGCHAIN_TRACING_V2\": os.environ.get(\"LANGCHAIN_TRACING_V2\", \"NOT SET\"),\n",
        "    \"LANGCHAIN_PROJECT\": os.environ.get(\"LANGCHAIN_PROJECT\", \"NOT SET\"),\n",
        "    \"LANGCHAIN_ENDPOINT\": os.environ.get(\"LANGCHAIN_ENDPOINT\", \"NOT SET\")\n",
        "}\n",
        "\n",
        "for key, value in env_vars.items():\n",
        "    if value == \"NOT SET\":\n",
        "        print(f\"âŒ {key}: {value}\")\n",
        "    elif key == \"LANGCHAIN_API_KEY\":\n",
        "        print(f\"âœ… {key}: {value[:8]}...{value[-4:] if len(value) > 12 else 'TOO SHORT'}\")\n",
        "    else:\n",
        "        print(f\"âœ… {key}: {value}\")\n",
        "\n",
        "# 2. Force set the environment variables again\n",
        "print(\"\\n2ï¸âƒ£ FORCE SETTING VARIABLES:\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Advanced-Retrieval-Debug\"\n",
        "print(f\"âœ… Set LANGCHAIN_TRACING_V2 = {os.environ['LANGCHAIN_TRACING_V2']}\")\n",
        "print(f\"âœ… Set LANGCHAIN_PROJECT = {os.environ['LANGCHAIN_PROJECT']}\")\n",
        "\n",
        "# 3. Test LangSmith client directly\n",
        "print(\"\\n3ï¸âƒ£ TESTING LANGSMITH CLIENT:\")\n",
        "try:\n",
        "    from langsmith import Client\n",
        "    client = Client()\n",
        "    print(f\"âœ… LangSmith Client created successfully\")\n",
        "    print(f\"âœ… API URL: {client.api_url}\")\n",
        "    \n",
        "    # Try to list projects to test connection\n",
        "    projects = list(client.list_projects(limit=5))\n",
        "    print(f\"âœ… Can connect to LangSmith - found {len(projects)} projects\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ LangSmith Client failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Use EXACTLY the same setup as your working France test\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "print(\"ðŸ§ª Using EXACT same pattern as working France test...\")\n",
        "\n",
        "# Create components exactly like France test\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "test_llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "test_prompt = ChatPromptTemplate.from_template(\"Answer this question: {question}\")\n",
        "test_chain = test_prompt | test_llm\n",
        "\n",
        "# Test with your loan question\n",
        "question = \"What is the most common issue with loans?\"\n",
        "print(f\"Testing with: {question}\")\n",
        "\n",
        "start_time = time.time()\n",
        "result = test_chain.invoke({\"question\": question})\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"âœ… Test completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"ðŸ“ Result: {result.content}\")\n",
        "print(\"\\nðŸ”— Check LangSmith - this should appear like France did!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - Advanced Retrieval - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"LangSmith API Key: \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
