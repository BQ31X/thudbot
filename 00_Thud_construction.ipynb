{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc5f067",
   "metadata": {},
   "source": [
    "# Thudbot Scaffold\n",
    "\n",
    "This will be a prototype Thudbot built in jupyter.\n",
    "\n",
    "Steps:\n",
    "1. General setup\n",
    "2. Data collection and Preparation\n",
    "3. SDG with RAGAS to create a golden data set\n",
    "4. Setup the RAG chain (finally)\n",
    "5. Evaluate results with RAGAS\n",
    "6. Refine RAG performance (prompt tuning, retreival methods)\n",
    "\n",
    "Once everything works:\n",
    "- Convert to a standalone Python script\n",
    "- Build or reuse a chatbot front end to run it locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5c4c3",
   "metadata": {},
   "source": [
    "\n",
    "Naming this 00_ so that it will be the first notebook every time I start a new session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badc3b1",
   "metadata": {},
   "source": [
    "## Step 1 General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182642a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- API Key Status ---\n",
      "OPENAI_API_KEY loaded: True\n",
      "LANGCHAIN_API_KEY loaded: True\n",
      "TAVILY_API_KEY loaded: True\n",
      "RAGAS_API_KEY loaded: False\n",
      "ANTHROPIC_API_KEY loaded: True\n",
      "COHERE_API_KEY loaded: True\n",
      "\n",
      "--- Project Settings Status ---\n",
      "DEBUG mode enabled: True\n",
      "LangSmith Tracing V2 enabled: True\n",
      "LangChain Project Base: None\n",
      "LangChain Project: THUDBOT-CC\n"
     ]
    }
   ],
   "source": [
    "### API key management and environment variables\n",
    "\n",
    "### Reminder: Place .env file inside the root of the project folder so when calling the below from inside the notebook it should find the .env fule and load it inside the notebook environment\n",
    "### PLEASE ADD THIS `.env` FILE TO YOUR PROJECT'S `.gitignore` file before committing and pushing the changes to your remote repo, as it contains API Keys and Secrets in it\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# --- Verify API Keys ---\n",
    "print(\"--- API Key Status ---\")\n",
    "print(f\"OPENAI_API_KEY loaded: {'OPENAI_API_KEY' in os.environ}\")\n",
    "print(f\"LANGCHAIN_API_KEY loaded: {'LANGCHAIN_API_KEY' in os.environ}\")\n",
    "print(f\"TAVILY_API_KEY loaded: {'TAVILY_API_KEY' in os.environ}\")\n",
    "print(f\"RAGAS_API_KEY loaded: {'RAGAS_API_KEY' in os.environ}\")\n",
    "print(f\"ANTHROPIC_API_KEY loaded: {'ANTHROPIC_API_KEY' in os.environ}\")\n",
    "print(f\"COHERE_API_KEY loaded: {'COHERE_API_KEY' in os.environ}\")\n",
    "\n",
    "# --- Verify General Settings ---\n",
    "print(\"\\n--- Project Settings Status ---\")\n",
    "print(f\"DEBUG mode enabled: {os.environ.get('DEBUG') == 'True'}\")\n",
    "print(f\"LangSmith Tracing V2 enabled: {os.environ.get('LANGCHAIN_TRACING_V2') == 'true'}\")\n",
    "print(f\"LangChain Project Base: {os.environ.get('LANGCHAIN_PROJECT_BASE')}\")\n",
    "print(f\"LangChain Project: {os.environ.get('LANGCHAIN_PROJECT')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09fc64",
   "metadata": {},
   "source": [
    "including nltk, because it worked before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140254c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/family/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/family/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06eb6339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/family/Library/Mobile Documents/com~apple~CloudDocs/AppDev/thudbot/.venv/lib/python3.13/site-packages/pysbd/segmenter.py:66: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  for match in re.finditer('{0}\\s*'.format(re.escape(sent)), self.original_text):\n",
      "/Users/family/Library/Mobile Documents/com~apple~CloudDocs/AppDev/thudbot/.venv/lib/python3.13/site-packages/pysbd/lang/arabic.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  txt = re.sub('(?<={0})\\.'.format(am), '‚àØ', txt)\n",
      "/Users/family/Library/Mobile Documents/com~apple~CloudDocs/AppDev/thudbot/.venv/lib/python3.13/site-packages/pysbd/lang/persian.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  txt = re.sub('(?<={0})\\.'.format(am), '‚àØ', txt)\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "eval_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "eval_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854ab91",
   "metadata": {},
   "source": [
    "## Step 2: Data Collection and Preparation\n",
    "\n",
    "My data is CSV structured, so using code from HW9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bf5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id: TSB-001\n",
      "hint_text: Press the escape key to exit the opening animations\n",
      "puzzle_name: \n",
      "source: self\n",
      "{'source': './data/Thudbot_Hint_Data_1.csv', 'row': 0, 'question': 'How do I stop the opening movie', 'hint_level': '1', 'character': 'Player', 'speaker': '', 'narrative_context': 'Meta', 'planet': '', 'location': '', 'category': 'Meta', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/Thudbot_Hint_Data_1.csv\",\n",
    "    metadata_columns=[\n",
    "        \"question\",\n",
    "        \"hint_level\",\n",
    "        \"character\",\n",
    "        \"speaker\",\n",
    "        \"narrative_context\",\n",
    "        \"planet\",\n",
    "        \"location\",\n",
    "        \"category\",\n",
    "        \"tone\",\n",
    "        \"follow_up_hint_id\",\n",
    "        \"answer_keywords\",\n",
    "        \"tags\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "hint_data = loader.load()\n",
    "\n",
    "# No need to overwrite page_content; not doing custom transformation\n",
    "print(hint_data[0].page_content)     # This will already be the hint_text\n",
    "print(hint_data[0].metadata)         # This will show all the metadata fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83858935",
   "metadata": {},
   "source": [
    "### Setting up QDrant! (from HW9)\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"ThudbotHints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e047a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=hint_data,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Thudbot_Hints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42404f5",
   "metadata": {},
   "source": [
    "## Step 3: Synthetic data generation (SDG) with RAGAS to create a golden dataset\n",
    "\n",
    "Adapted from HW 7 & 9\n",
    "\n",
    "NOTE: this step is a one-time deal. Once the appropriate golden data set is finalized, don't run any of these cells, but pick up at the JSON import cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16656100",
   "metadata": {},
   "source": [
    "### ‚è≠Ô∏è Skip these steps until/unless want new golden data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156621a",
   "metadata": {},
   "source": [
    "#### Group data by narrative context\n",
    "\n",
    "the data is too granular for Ragas, I got this error:\n",
    "\n",
    "ValueError: Documents appears to be too short (ie 100 tokens or less). Please provide longer documents\n",
    "\n",
    "so,  I will to group it and \n",
    "Create a new list called  ```merged_docs``` , just to feed to ragas, but not for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9037046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group hints by narrative context\n",
    "grouped = defaultdict(list)\n",
    "\n",
    "for doc in hint_data:\n",
    "    key = doc.metadata.get(\"narrative_context\", \"unknown\")\n",
    "    grouped[key].append(doc.page_content)\n",
    "\n",
    "# Create longer Documents for SDG\n",
    "merged_docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join(hints),\n",
    "        metadata={\"narrative_context\": context}\n",
    "    )\n",
    "    for context, hints in grouped.items()\n",
    "]\n",
    "\n",
    "# Optional: preview length\n",
    "for doc in merged_docs:\n",
    "    print(f\"{doc.metadata['narrative_context']}: {len(doc.page_content.split())} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e012e8d",
   "metadata": {},
   "source": [
    "Need to select a random subset, because the data is grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "import random\n",
    "\n",
    "sampled_docs = random.sample(merged_docs, 5)\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "golden_dataset = generator.generate_with_langchain_docs(sampled_docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b52029",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4da258",
   "metadata": {},
   "source": [
    "save the RAGAS golden data set as a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/goldendataset.json\", \"w\") as f:\n",
    "    json.dump([sample.model_dump() for sample in golden_dataset], f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f64552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export Golden Dataset to Human-Readable Formats\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the golden dataset\n",
    "with open('./data/goldendataset.json', 'r') as f:\n",
    "    golden_data = json.load(f)\n",
    "\n",
    "# Extract the key information into a clean DataFrame\n",
    "readable_data = []\n",
    "for i, item in enumerate(golden_data):\n",
    "    eval_sample = item['eval_sample']\n",
    "    \n",
    "    # Clean up the reference contexts (truncate if too long)\n",
    "    contexts = eval_sample.get('reference_contexts', [])\n",
    "    contexts_preview = str(contexts[0])[:200] + \"...\" if contexts else \"No context\"\n",
    "    \n",
    "    readable_data.append({\n",
    "        'Question_ID': f\"Q{i+1:02d}\",\n",
    "        'User_Question': eval_sample.get('user_input', ''),\n",
    "        'Expected_Answer': eval_sample.get('reference', ''),\n",
    "        'Synthesizer_Type': item.get('synthesizer_name', ''),\n",
    "        'Context_Preview': contexts_preview,\n",
    "        'Full_Context_Length': len(str(contexts)) if contexts else 0\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(readable_data)\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv('./data/golden_dataset_readable.csv', index=False)\n",
    "\n",
    "# Display preview\n",
    "print(\"Golden Dataset Preview:\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in df.head(3).iterrows():\n",
    "    print(f\"Question {row['Question_ID']}: {row['User_Question']}\")\n",
    "    print(f\"Expected: {row['Expected_Answer']}\")\n",
    "    print(f\"Synthesizer: {row['Synthesizer_Type']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n‚úÖ Exported {len(df)} questions to: ./data/golden_dataset_readable.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9551336",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 2: Create Detailed Markdown Report\n",
    "\n",
    "def create_markdown_report(golden_data, output_file='./data/golden_dataset_report.md'):\n",
    "    \"\"\"Create a detailed, human-readable markdown report of the golden dataset.\"\"\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# Thudbot Golden Dataset Report\\n\\n\")\n",
    "        f.write(\"Generated synthetic test questions and answers for RAGAS evaluation.\\n\\n\")\n",
    "        f.write(f\"**Total Questions:** {len(golden_data)}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        for i, item in enumerate(golden_data, 1):\n",
    "            eval_sample = item['eval_sample']\n",
    "            \n",
    "            f.write(f\"## Question {i:02d}\\n\\n\")\n",
    "            f.write(f\"**User Input:** {eval_sample.get('user_input', 'N/A')}\\n\\n\")\n",
    "            f.write(f\"**Expected Answer:**\\n{eval_sample.get('reference', 'N/A')}\\n\\n\")\n",
    "            f.write(f\"**Synthesizer:** `{item.get('synthesizer_name', 'unknown')}`\\n\\n\")\n",
    "            \n",
    "            # Add context information\n",
    "            contexts = eval_sample.get('reference_contexts', [])\n",
    "            if contexts:\n",
    "                f.write(\"**Reference Context Preview:**\\n\")\n",
    "                context_preview = str(contexts[0])[:300] + \"...\" if len(str(contexts[0])) > 300 else str(contexts[0])\n",
    "                f.write(f\"```\\n{context_preview}\\n```\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Created detailed report: {output_file}\")\n",
    "\n",
    "# Generate the markdown report\n",
    "create_markdown_report(golden_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e7aad",
   "metadata": {},
   "source": [
    "the answers are okay, but the questions are crappy. Let's see if we can get an LLM to rewrite the questions in a more natural game-player voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0010f",
   "metadata": {},
   "source": [
    "##### Five step plan to re-write the questions:\n",
    "\n",
    "1. Define rewriter prompt\n",
    "2. Set up a rewriter chain using the rewriter prompt\n",
    "3. Define rewrite function that invokes rewriter_chain\n",
    "4. Run the function on the original \"golden\" dataset\n",
    "5. Write the re-written questions to .json and .md files\n",
    "\n",
    "\"Vibe-check\" the output, and iterate until I like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee97c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question Rewriter: Transform Formal Questions ‚Üí Casual Player Questions\n",
    "\n",
    "# Using the same pattern as HW7 for LLM chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the rewriter prompt (following HW7 prompt pattern)\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "QUESTION_REWRITER_PROMPT = \"\"\"\\\n",
    "You are helping rewrite questions to sound like real confused players asking for game hints.\n",
    "\n",
    "Your job is to transform formal/academic questions into casual, human-like questions that real players would ask when stuck in \"The Space Bar\" adventure game.\n",
    "\n",
    "IMPORTANT: If the question contains a question ID like \"TSB-041\" or similar, ignore that completely - it's just an organizational code that doesn't exist in the real game.\n",
    "\n",
    "Original question: {original_question}\n",
    "Game context: {context_preview}\n",
    "\n",
    "Examples of good player questions:\n",
    "- \"How do I get past the guard?\"\n",
    "- \"I'm stuck in this room, what do I do?\"\n",
    "- \"Help! I can't figure out this puzzle\"\n",
    "- \"Where's the token I need?\"\n",
    "\n",
    "Make it sound confused, or casual - like a real person, not an academic paper.\n",
    "\n",
    "Rewritten question:\"\"\"\n",
    "\n",
    "# Create the prompt template (same pattern as HW7)\n",
    "rewriter_prompt = ChatPromptTemplate.from_template(QUESTION_REWRITER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up a rewriter chain using the rewriter prompt\n",
    "\n",
    "# Set up the LLM for rewriting (following HW7 pattern)\n",
    "rewriter_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# Create the rewriter chain (same LCEL pattern as HW7)\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "rewriter_chain = rewriter_prompt | rewriter_llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define rewrite function that invokes rewriter_chain\n",
    "# Main rewriting function (following HW7 iteration pattern)\n",
    "def rewrite_questions_to_player_style(golden_data):\n",
    "    \"\"\"Rewrite formal RAGAS questions to sound like casual game players\"\"\"\n",
    "    \n",
    "    updated_data = []\n",
    "    \n",
    "    print(\"üîÑ Rewriting questions to sound like real players...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, item in enumerate(golden_data, 1):\n",
    "        eval_sample = item['eval_sample']\n",
    "        original_q = eval_sample.get('user_input', '')\n",
    "        \n",
    "        # Get context preview for better rewriting\n",
    "        contexts = eval_sample.get('reference_contexts', [])\n",
    "        context_preview = str(contexts[0])[:200] + \"...\" if contexts else \"No context available\"\n",
    "        \n",
    "        # Rewrite the question using our chain\n",
    "        try:\n",
    "            rewritten_q = rewriter_chain.invoke({\n",
    "                \"original_question\": original_q,\n",
    "                \"context_preview\": context_preview\n",
    "            })\n",
    "            \n",
    "            # Update the question in place\n",
    "            eval_sample['user_input'] = rewritten_q.strip()\n",
    "            eval_sample['original_question'] = original_q  # Keep original for comparison\n",
    "            \n",
    "            # Show progress\n",
    "            print(f\"‚úÖ Question {i:02d}:\")\n",
    "            print(f\"   Original: {original_q[:70]}...\")\n",
    "            print(f\"   Rewritten: {rewritten_q.strip()[:70]}...\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error rewriting question {i}: {e}\")\n",
    "            # Keep original if rewriting fails\n",
    "            eval_sample['original_question'] = original_q\n",
    "        \n",
    "        updated_data.append(item)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully rewrote {len(updated_data)} questions!\")\n",
    "    return updated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c249a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Run the re on the original \"golden\" dataset\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the original golden dataset\n",
    "with open('./data/goldendataset.json', 'r') as f:\n",
    "    original_golden_data = json.load(f)\n",
    "\n",
    "print(f\"üìä Loaded {len(original_golden_data)} questions to rewrite\")\n",
    "\n",
    "# Rewrite the questions\n",
    "rewritten_data = rewrite_questions_to_player_style(original_golden_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31116011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the rewritten dataset and create comparison report\n",
    "with open('./data/goldendataset_player_style.json', 'w') as f:\n",
    "    json.dump(rewritten_data, f, indent=2)\n",
    "\n",
    "# Create a comparison report to review the changes\n",
    "def create_comparison_report(rewritten_data, output_file='./data/question_rewrite_comparison.md'):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# Question Rewriting Comparison Report\\n\\n\")\n",
    "        f.write(\"Comparison of original RAGAS questions vs. player-style questions\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        for i, item in enumerate(rewritten_data, 1):\n",
    "            eval_sample = item['eval_sample']\n",
    "            original = eval_sample.get('original_question', 'N/A')\n",
    "            rewritten = eval_sample.get('user_input', 'N/A')\n",
    "            \n",
    "            f.write(f\"## Question {i:02d}\\n\\n\")\n",
    "            f.write(f\"**Original (RAGAS):** {original}\\n\\n\")\n",
    "            f.write(f\"**Rewritten (Player Style):** {rewritten}\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    print(f\"üìù Created comparison report: {output_file}\")\n",
    "\n",
    "create_comparison_report(rewritten_data)\n",
    "print(f\"üíæ Saved rewritten dataset to: ./data/goldendataset_player_style.json\")\n",
    "print(\"\\nüéØ Ready to use the player-style questions for RAGAS evaluation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908666af",
   "metadata": {},
   "source": [
    "I ran a few iterations with different prompts, and an now reasonably happy with the questions. \n",
    "\n",
    "Next step is to import the ```goldendataset_player_style.json``` for use by ragas eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cacdfd8",
   "metadata": {},
   "source": [
    "#### End of SDG work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36831f3",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è Resume here to load data for any RAGAS eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15427af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load questions for testing your retrievers\n",
    "with open(\"data/goldendataset_player_style.json\", \"r\") as f:\n",
    "    golden_data = json.load(f)\n",
    "\n",
    "def run_retriever_on_dataset(name, retriever_chain, golden_data):\n",
    "    \"\"\"Run retriever and format for Ragas evaluation - consistent with your existing function\"\"\"\n",
    "    print(f\"Running {name} on golden dataset\")\n",
    "    outputs = []\n",
    "    \n",
    "    for item in golden_data:\n",
    "        question = item[\"eval_sample\"][\"user_input\"]\n",
    "        reference = item[\"eval_sample\"][\"reference\"]\n",
    "        \n",
    "        # Run your retriever\n",
    "        response = retriever_chain.invoke({\"question\": question})\n",
    "        \n",
    "        outputs.append({\n",
    "            \"user_input\": question,\n",
    "            \"reference\": reference,\n",
    "            \"response\": response[\"response\"].content if hasattr(response[\"response\"], \"content\") else response[\"response\"],\n",
    "            \"retrieved_contexts\": [ctx.page_content for ctx in response[\"context\"]],\n",
    "            \"retriever_name\": name\n",
    "        })\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017455bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb2736",
   "metadata": {},
   "source": [
    "## Step 4: Setup the RAG chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc850ac",
   "metadata": {},
   "source": [
    "Starting with a \"naive\" dense vector retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274cae5",
   "metadata": {},
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67ec143",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781088a",
   "metadata": {},
   "source": [
    "### A - Augmented\n",
    "\n",
    "My first pass at a Thud-like prompt, named as ```THUD_TEMPLATE```\n",
    "\n",
    "This will need tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a920e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "THUD_TEMPLATE = \"\"\"\\\n",
    "You are Thud, a friendly and somewhat simple-minded patron at The Thirsty Tentacle. \n",
    "\n",
    "You're trying your best to help the player navigate the game \"The Space Bar.\"\n",
    "\n",
    "Use the clues and context provided below to offer a gentle hint ‚Äî not a full solution.\n",
    "\n",
    "If you're not sure, say so, or suggest the player look around more.\n",
    "\n",
    "Player's question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Your hint:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(THUD_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e84599",
   "metadata": {},
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27603ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "#chat_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43372065",
   "metadata": {},
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e85297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    \n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    \n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc3681",
   "metadata": {},
   "source": [
    "Test the chain, and the langsmith tracing with a question.\n",
    "Might as well take the question from the golden data set (just remember to load it above ‚ñ∂Ô∏è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17b91e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': AIMessage(content=\"Well, Yzore's a tricky place, but I think the Yzore helps you because you need a special token to hop on a bus there. Maybe there's something at Glom Hole or around that mailbox that can help you get it. Keep looking around in those spots‚Äîyou might find a clue or item that'll get you closer to your goal!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 2067, 'total_tokens': 2138, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C0FBkhEpUPOjVrDBoI6Q5mzP4sJbT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0cbf6144-0850-4f7c-818a-32c915fc6738-0', usage_metadata={'input_tokens': 2067, 'output_tokens': 71, 'total_tokens': 2138, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'context': [Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 78, 'question': 'How do I get on the bus to Quantelope Lodge?', 'hint_level': '1', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Front Stoop', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'd968e09b6ae5446bb648231242a38acc', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-046\\nhint_text: You need a token to get on a bus in Yzore\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 86, 'question': 'Thud got on the wrong bus.', 'hint_level': '2', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Glom Hole', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': '8edb39d061194a56a3ed96d9c2c29d90', '_collection_name': 'Thudbot_Hints'}, page_content=\"question_id: TSB-048\\nhint_text: Maybe there's something you can do at Glom Hole?\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS\"),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 76, 'question': 'How do I get to the Quantelope lodge', 'hint_level': '1', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Front Stoop', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'e4e99c1fe7ec46ae9abf99bea4ff7768', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-045\\nhint_text: To get to the Quantelpoe Lodge, Thud and Fleebix should take a bus\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 70, 'question': 'How do I find Thud when I am in a jar?', 'hint_level': '1', 'character': 'Fleebix', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Front Stoop', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': '5ae286f1d89d4256ad8169c3faf34c9f', '_collection_name': 'Thudbot_Hints'}, page_content=\"question_id: TSB-042\\nhint_text: I could tell you how to find Thud, but maybe you should EmpTel with Thud first if you haven't\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS\"),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 74, 'question': 'What do I need to do in the Fleebix  flashback?', 'hint_level': '2', 'character': 'Fleebix', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': '', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': '057ea13907b449de823c8c8a30be8145', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-044\\nhint_text: Read the audition notice to find out where Thud and Fleebix need to go first\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 75, 'question': 'What do I need to do in the Fleebix  flashback?', 'hint_level': '3', 'character': 'Fleebix', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': '', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'bd32f769dddd4552a4caa936eef7b286', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-044\\nhint_text: Thud and Fleebix need to get to the Quantelope Lodge\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 102, 'question': 'What can I do inside the mail truck?', 'hint_level': '1', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Mail truck', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'd4cd51f7c55e4cb2a0ace6df3947dbdc', '_collection_name': 'Thudbot_Hints'}, page_content=\"question_id: TSB-054\\nhint_text: Not much to do here. Look around the truck. Don't get caught.\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS\"),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 88, 'question': 'How do I get to Quantelope Lodge from Glom Hole', 'hint_level': '2', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Glom Hole', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': '8b877f074dfe43df887db20970a355b2', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-049\\nhint_text: Have you looked at the mailbox near Glom Hole?\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 69, 'question': \"Why can't I do anything when I EmpTel as Fleebix\", 'hint_level': '2', 'character': 'Fleebix', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': '', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'e7adb477f8c14466b25320ee4f3159be', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-041\\nhint_text: When you are Fleebix, you can rely on Thud to help you with many taks\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS'),\n",
       "  Document(metadata={'source': './data/Thudbot_Hint_Data_1.csv', 'row': 106, 'question': 'What can I do inside the mail truck?', 'hint_level': '4', 'character': 'Fleebix, Thud', 'speaker': '', 'narrative_context': 'Fleebix Flashback', 'planet': 'Yzore', 'location': 'Cardboard Box', 'category': 'Puzzle', 'tone': '', 'follow_up_hint_id': '', 'answer_keywords': '', 'tags': '', '_id': 'af04010d0cac453792fb997aad11a03a', '_collection_name': 'Thudbot_Hints'}, page_content='question_id: TSB-054\\nhint_text: Wait for the truck to deliver you\\npuzzle_name: Getting to Quantelope Lodge\\nsource: UHS')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_q = golden_data[0][\"eval_sample\"][\"user_input\"]\n",
    "naive_retrieval_chain.invoke({\"question\": sample_q})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb0b03",
   "metadata": {},
   "source": [
    "#### RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd769b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_outputs = run_retriever_on_dataset(\"bm25_thing\", bm25_retrieval_chain, golden_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated version of the code, with the retriever name externalized\n",
    "# import time\n",
    "\n",
    "def run_retriever_on_dataset(name, retriever_chain, golden_dataset):\n",
    "    print(f\"Running {name} on golden dataset\")\n",
    "    outputs = []\n",
    "\n",
    "    for test_row in golden_dataset:\n",
    "        response = retriever_chain.invoke({\n",
    "            \"question\": test_row.eval_sample.user_input})\n",
    "        outputs.append({\n",
    "            \"user_input\": test_row.eval_sample.user_input,\n",
    "            \"reference\": test_row.eval_sample.reference,\n",
    "            \"response\": response[\"response\"].content if hasattr(response[\"response\"], \"content\") else response[\"response\"],\n",
    "            \"retrieved_contexts\": [ctx.page_content for ctx in response[\"context\"]],\n",
    "            \"retriever_name\": name #this is the new addition for being able to keep track of the retriever name later\n",
    "        })\n",
    "\n",
    "\n",
    "    #    # Add delay between requests\n",
    "    #     if i < len(golden_dataset) - 1:  # Don't sleep after last item\n",
    "    #         print(f\"  Waiting 2 seconds before next request...\")\n",
    "    #         time.sleep(2)  # Adjust this value as needed\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25_outputs = run_retriever_on_dataset(\"bm25\", bm25_retrieval_chain, golden_dataset) #this was the first version, hard-coded\n",
    "# naive_outputs = run_retriever_on_dataset(\"naive\", naive_retrieval_chain, golden_dataset)\n",
    "multi_query_outputs = run_retriever_on_dataset(\"multi_query\", multi_query_retrieval_chain, golden_dataset)\n",
    "# parent_doc_outputs = run_retriever_on_dataset(\"parent_doc\", parent_document_retrieval_chain, golden_dataset)\n",
    "# ensemble_outputs = run_retriever_on_dataset(\"ensemble\", ensemble_retrieval_chain, golden_dataset)\n",
    "# contextual_compression_outputs = run_retriever_on_dataset(\"contextual_compression\", contextual_compression_retrieval_chain, golden_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
